{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Assignment 2 — Singing Voice Transcription  \n",
    "**Weight:** 15 % of final grade (48 marks total)\n",
    "\n",
    "Welcome to the second assignment of this course. In this exercise, you will build a **Singing Voice Transcription (SVT)** system for songs—a system that converts singing voice audio into note-level notation. The assignment consists of three sections:\n",
    "\n",
    "1. Building the necessary components for the system  \n",
    "2. Implementing the complete transcription workflow  \n",
    "3. Answering extension questions  \n",
    "\n",
    "### Requirements\n",
    "- Complete this notebook by running all code cells successfully and answering all questions.  \n",
    "- When embedding a screenshot in the notebook, place the image file in the `./resources` directory.  \n",
    "- After finishing, **zip the entire assignment directory** (excluding the `data_mini` directory) and submit it to Canvas.  \n",
    "\n",
    "### Submission \n",
    "Upon completing this assignment, please **compress the entire assignment directory** into a `.zip` file and submit it via Canvas. File naming convention: **`eXXXXXXX_Name_Assignment2.zip`**.\n",
    "\n",
    "### Late Policy \n",
    "A penalty of **25% of the total marks per day** will be applied for late submissions. All assignments must be submitted via Canvas and we do not accept email submissions.\n",
    "\n",
    "### Honor Code\n",
    "Plagiarism will not be tolerated. You may discuss the questions with classmates and consult online references, but you must not submit code or answers copied directly from other sources. If you refer to code, algorithms, or tutorials from external sources, you must explicitly acknowledge the source in your submission (e.g., in code comments).\n",
    "\n",
    "This assignment is worth **15 %** of your final grade. The full mark for this notebook is **48**. Your score will be normalized when computing the final grade: **Your assignment 2 score = [Your Score] / 48 * 15**.\n",
    "\n",
    "### Notes\n",
    "1. **Restart the Kernel After File Changes**  \n",
    "   After modifying any `.py` file, restart the Jupyter kernel to ensure the updated code is loaded. Restarting will not erase previous cell outputs, so your execution history remains available.  \n",
    "\n",
    "2. **Maintain Tensor Shape Consistency**  \n",
    "   When reshaping or altering tensor dimensions, ensure all subsequent operations remain shape-compatible to avoid runtime errors or incorrect results.  \n",
    "\n",
    "3. **Restricted Editing Scope**  \n",
    "   You may only add or modify code within sections explicitly marked `YOUR CODE:`. Do not alter any other parts of the provided framework.  \n",
    "\n",
    "### Useful Resources\n",
    "- [Music Transcription Overview](https://www.eecs.qmul.ac.uk/~simond/pub/2018/BenetosDixonDuanEwert-SPM2018-Transcription.pdf)  \n",
    "- [Evaluation for Singing Transcription](https://riuma.uma.es/xmlui/bitstream/handle/10630/8372/298_Paper.pdf?sequence=1)  \n",
    "- [mir_eval Documentation](https://craffel.github.io/mir_eval/#mir_eval.transcription.evaluate)  \n",
    "- [VOCANO: A Note Transcription Framework for Singing Voice in Polyphonic Music](https://archives.ismir.net/ismir2021/paper/000036.pdf)  \n",
    "- [TONet: Tone–Octave Network for Singing Melody Extraction from Polyphonic Music](http://arxiv.org/abs/2202.00951)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Getting started\n",
    "\n",
    "We recommend using a **Conda** environment for this course. If you have not yet set one up, follow the steps below:\n",
    "\n",
    "1. **Install Miniconda**  \n",
    "- Download [Miniconda](https://docs.conda.io/en/latest/miniconda.html) and install it on your system.  \n",
    "- After installation, restart your terminal (PowerShell, zsh, or bash).  \n",
    "\n",
    "2. **Verify Installation**  \n",
    "- When `(base)` appears at the start of your terminal prompt, Miniconda is active.  \n",
    "     ![Conda Base Environment](./resources/conda.png)\n",
    "\n",
    "3. **Create and Activate the Course Environment**  \n",
    "   ```bash\n",
    "   conda create -n 4347 python=3.9\n",
    "   conda activate 4347\n",
    "\n",
    "4. **Install packages**\n",
    "\n",
    "        # Install PyTorch (use command suitable for your OS)\n",
    "        # Linux / Windows (CUDA 11.7)\n",
    "        pip install torch==2.0.0+cu117 torchvision==0.15.1+cu117 torchaudio==2.0.1 --index-url https://download.pytorch.org/whl/cu117\n",
    "        # Windows (CPU)\n",
    "        pip install torch==2.0.0+cpu torchvision==0.15.1+cpu torchaudio==2.0.1 --index-url https://download.pytorch.org/whl/cpu\n",
    "        # OSX (CPU)\n",
    "        pip install torch==2.0.0 torchvision==0.15.1 torchaudio==2.0.1\n",
    "\n",
    "        # Install other libraries\n",
    "        pip install -r requirement.txt\n",
    "\n",
    "        # Install ffmpeg\n",
    "        conda install ffmpeg\n",
    "\n",
    "5. **Set Interpreter in Your IDE**\n",
    "- When running this notebook in your IDE, ensure the interpreter is set to the 4347 Conda environment.\n",
    "\n",
    "6. **Install Jupyter if Prompted**\n",
    "- If prompted to install the jupyter package, click **Confirm** to proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Section 1 - Important Components [21 marks]\n",
    "We are going to use pytorch to build a neural network for SVT. Like all the deep learning projects, we start with data pipeline.\n",
    "\n",
    "### Task 1: Data Loader [3 marks]\n",
    "\n",
    "**YOUR TASK:** Complete the `MyDataset` class in **`dataset.py`** so that it passes the test in the provided cell. **[3 marks]**\n",
    "\n",
    "You need to fill the code that implement:\n",
    "1. Convert note-level annotations to frame-level annotations.\n",
    "2. Load the audio file and convert it to a [mel spectrogram](https://en.wikipedia.org/wiki/Mel-frequency_cepstrum). Some tools may help you, like [torchaudio](https://pytorch.org/audio/main/generated/torchaudio.transforms.MelSpectrogram.html) or [librosa](https://librosa.org/doc/main/generated/librosa.feature.melspectrogram.html).\n",
    "3. Extract 5-s segments from both the spectrogram and the corresponding annotations to create training samples.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/123 [00:20<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/Users/charlie/miniconda3/envs/4347/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/Users/charlie/miniconda3/envs/4347/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/Users/charlie/miniconda3/envs/4347/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/Users/charlie/University/3A/Sound and Music Computing/Assignments/assignment2/dataset.py\", line 141, in __getitem__\n    waveform, sample_rate = torchaudio.load(audio_fp)\n  File \"/Users/charlie/miniconda3/envs/4347/lib/python3.9/site-packages/torchaudio/backend/sox_io_backend.py\", line 256, in load\n    return _fallback_load(filepath, frame_offset, num_frames, normalize, channels_first, format)\n  File \"/Users/charlie/miniconda3/envs/4347/lib/python3.9/site-packages/torchaudio/backend/sox_io_backend.py\", line 30, in _fail_load\n    raise RuntimeError(\"Failed to load audio from {}\".format(filepath))\nRuntimeError: Failed to load audio from ./data_mini/train/60/Mixture.mp3\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 6\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mhparams\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Hparams\n\u001b[1;32m      5\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m get_data_loader(split\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m, args\u001b[38;5;241m=\u001b[39mHparams\u001b[38;5;241m.\u001b[39margs)\n\u001b[0;32m----> 6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader):\n\u001b[1;32m      7\u001b[0m     x, onset, offset, octave, pitch_class \u001b[38;5;241m=\u001b[39m move_data_to_device(data, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcpu\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      8\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m [\u001b[38;5;241m8\u001b[39m, \u001b[38;5;241m250\u001b[39m, \u001b[38;5;241m256\u001b[39m]  \u001b[38;5;66;03m# shape in [B, T, D],\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/4347/lib/python3.9/site-packages/tqdm/std.py:1182\u001b[0m, in \u001b[0;36mtqdm.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1179\u001b[0m time \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_time\n\u001b[1;32m   1181\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1182\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m obj \u001b[38;5;129;01min\u001b[39;00m iterable:\n\u001b[1;32m   1183\u001b[0m         \u001b[38;5;28;01myield\u001b[39;00m obj\n\u001b[1;32m   1184\u001b[0m         \u001b[38;5;66;03m# Update and possibly print the progressbar.\u001b[39;00m\n\u001b[1;32m   1185\u001b[0m         \u001b[38;5;66;03m# Note: does not call self.update(1) for speed optimisation.\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/4347/lib/python3.9/site-packages/torch/utils/data/dataloader.py:634\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    631\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    632\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    633\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 634\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    635\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    636\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    637\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    638\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/miniconda3/envs/4347/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1346\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1344\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1345\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_task_info[idx]\n\u001b[0;32m-> 1346\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_process_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/4347/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1372\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._process_data\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m   1370\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_try_put_index()\n\u001b[1;32m   1371\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(data, ExceptionWrapper):\n\u001b[0;32m-> 1372\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreraise\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1373\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/miniconda3/envs/4347/lib/python3.9/site-packages/torch/_utils.py:644\u001b[0m, in \u001b[0;36mExceptionWrapper.reraise\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m    641\u001b[0m     \u001b[38;5;66;03m# If the exception takes multiple arguments, don't try to\u001b[39;00m\n\u001b[1;32m    642\u001b[0m     \u001b[38;5;66;03m# instantiate since we don't know how to\u001b[39;00m\n\u001b[1;32m    643\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(msg) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 644\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m exception\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Caught RuntimeError in DataLoader worker process 0.\nOriginal Traceback (most recent call last):\n  File \"/Users/charlie/miniconda3/envs/4347/lib/python3.9/site-packages/torch/utils/data/_utils/worker.py\", line 308, in _worker_loop\n    data = fetcher.fetch(index)\n  File \"/Users/charlie/miniconda3/envs/4347/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in fetch\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/Users/charlie/miniconda3/envs/4347/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\", line 51, in <listcomp>\n    data = [self.dataset[idx] for idx in possibly_batched_index]\n  File \"/Users/charlie/University/3A/Sound and Music Computing/Assignments/assignment2/dataset.py\", line 141, in __getitem__\n    waveform, sample_rate = torchaudio.load(audio_fp)\n  File \"/Users/charlie/miniconda3/envs/4347/lib/python3.9/site-packages/torchaudio/backend/sox_io_backend.py\", line 256, in load\n    return _fallback_load(filepath, frame_offset, num_frames, normalize, channels_first, format)\n  File \"/Users/charlie/miniconda3/envs/4347/lib/python3.9/site-packages/torchaudio/backend/sox_io_backend.py\", line 30, in _fail_load\n    raise RuntimeError(\"Failed to load audio from {}\".format(filepath))\nRuntimeError: Failed to load audio from ./data_mini/train/60/Mixture.mp3\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "from dataset import get_data_loader, move_data_to_device\n",
    "from hparams import Hparams\n",
    "\n",
    "train_loader = get_data_loader(split='train', args=Hparams.args)\n",
    "for data in tqdm(train_loader):\n",
    "    x, onset, offset, octave, pitch_class = move_data_to_device(data, 'cpu')\n",
    "    assert list(x.shape) == [8, 250, 256]  # shape in [B, T, D],\n",
    "                                # i.e., [Batch size, num of frame per sample, spectrogram feature dimension]\n",
    "    assert list(onset.shape) == list(offset.shape) == list(octave.shape) == list(pitch_class.shape) == [8, 250]\n",
    "    break\n",
    "print('Congrats!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 2: Speed it up  [7 marks]\n",
    "\n",
    "At this stage, your data loader should be functional, but it may still have performance bottlenecks.  \n",
    "In this task, we will evaluate its efficiency by loading **5 batches** of data from your data loader.  \n",
    "\n",
    "**YOUR TASK:**  Run the provided code cell below to load 5 batches and observe the execution time. **[1 mark]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from dataset import get_data_loader, move_data_to_device\n",
    "from hparams import Hparams\n",
    "import time\n",
    "\n",
    "t0 = time.time()\n",
    "train_loader = get_data_loader(split='train', args=Hparams.args)\n",
    "for i, data in enumerate(tqdm(train_loader)):\n",
    "    x, onset, offset, octave, pitch_class = move_data_to_device(data, 'cpu')\n",
    "    assert list(x.shape) == [8, 250, 256]  # shape in [B, T, D], i.e., [Batch size, num of frame per sample, feature dimention]\n",
    "    assert list(onset.shape) == list(offset.shape) == list(octave.shape) == list(pitch_class.shape) == [8, 250]\n",
    "    if i == 4:\n",
    "        dur = time.time()-t0\n",
    "        est_time = dur / 5 * 123\n",
    "        break\n",
    "print('5 batches use {:.2f} seconds'.format(dur))\n",
    "print('Estimated time to load the whole training set: {:.2f} seconds.'.format(est_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "If the estimated time to load the entire training set is **within 30 seconds**, congratulations — your data loader is efficient.  \n",
    "If you have not modified the dataset workflow, it may take **over 1000 seconds** to load the entire set, making data loading the primary performance bottleneck.\n",
    "\n",
    "**YOUR TASK:**\n",
    "\n",
    "1. **Answer the two questions below.** **[2 marks]**  \n",
    "2. **Optimize your `MyDataset` class** to reduce data loading time. **[3 marks]**  \n",
    "   *Hint:*  \n",
    "   - Each dataset iteration should return only a 5-second audio clip. The current implementation reads the entire audio file into memory each time, even if only one clip is needed. If the same file is accessed again, the I/O operation could be skipped.  \n",
    "   - Frame-level annotations are currently recomputed every time a clip is loaded; this can be cached or precomputed to improve efficiency.  \n",
    "   - Consider methods to read only the required audio segment directly from disk rather than the entire file. You may also change the storage format of audio files if it improves performance.  \n",
    "3. **Restart the kernel** and run the provided code cell to verify the improvement. **[1 mark]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Question:\n",
    "\n",
    "[1] What are the primary factors contributing to the time overhead in the current data-loading process?\n",
    "**[Your Answer Here]**\n",
    "\n",
    "[2] What is your plan for modifying the dataset class? Propose two possible solutions to improve its efficiency. Indicate which solution you will implement and explain your choice, including the expected benefits and trade-offs.\n",
    "**[Your Answer Here]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from dataset import get_data_loader, move_data_to_device\n",
    "from hparams import Hparams\n",
    "import time\n",
    "\n",
    "train_loader = get_data_loader(split='train', args=Hparams.args)\n",
    "t0 = time.time()\n",
    "for i, data in enumerate(tqdm(train_loader)):\n",
    "    x, onset, offset, octave, pitch_class = move_data_to_device(data, 'cpu')\n",
    "    assert list(x.shape) == [8, 250, 256]  # shape in [B, T, D], i.e., [Batch size, num of frame per sample, feature dimention]\n",
    "    assert list(onset.shape) == list(offset.shape) == list(octave.shape) == list(pitch_class.shape) == [8, 250]\n",
    "    if i == 4:\n",
    "        dur = time.time()-t0\n",
    "        est_time = dur / 5 * 123\n",
    "        break\n",
    "print('5 batches use {:.2f} seconds'.format(dur))\n",
    "print('Estimated time to load the whole training set: {:.2f} seconds.'.format(est_time))\n",
    "if est_time < 40:\n",
    "    print('Well Done!')\n",
    "else:\n",
    "    print('We can still be faster.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 3: Loss function [3 marks]\n",
    "\n",
    "We are going to use \"multitask learning\" to train our model. I.e., we are training our model simultaneously on 4 tasks: 4 types of frame classification tasks. They are:\n",
    "- Classify if there is an onset on some frame.\n",
    "- Classify if there is an offset on some frame.\n",
    "- Classify the pitch of some frame lies on which octave (we have 4 octaves numbered 1~4, and 0 means silence).\n",
    "- Classify which pitch class if the current pitch (from C~B, 12 semitones, and 0 which means silence).\n",
    "\n",
    "In this case, the loss function is not that straightforward. To improve the readability of our code, we are going wrap the loss computation into a class.\n",
    "\n",
    "**YOUR TASK:** Finish the code of train.LossFunc class, and run the cell below.  **[3 mark(s)]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from train import LossFunc\n",
    "\n",
    "loss_func = LossFunc(device='cpu')\n",
    "\n",
    "on_out = torch.rand(size=(8, 250))          # [B, T]\n",
    "off_out = torch.rand(size=(8, 250))\n",
    "octave_out = torch.rand(size=(8, 250, 5))   # [B, T, #Class]\n",
    "pitch_class_out = torch.rand(size=(8, 250, 13))\n",
    "\n",
    "on_tgt = on_out\n",
    "off_tgt = off_out\n",
    "octave_tgt = torch.randint(high=5, size=(8, 250))\n",
    "pitch_class_tgt = torch.randint(high=13, size=(8, 250))\n",
    "\n",
    "\n",
    "losses = loss_func.get_loss(\n",
    "    out=(on_out, off_out, octave_out, pitch_class_out),\n",
    "    tgt=(on_tgt, off_tgt, octave_tgt, pitch_class_tgt)\n",
    ")\n",
    "\n",
    "assert losses != None\n",
    "assert len(losses) == 5\n",
    "assert isinstance(losses[0], torch.Tensor)\n",
    "print('Succeed!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 4: Metric [3 marks]\n",
    "\n",
    "We need to observe the model performance during training to see how the training is going on. In addition to loss, we may also want to know the f1 score or accuracy, in both training loop and validation loop.\n",
    "To facilitate this, we wrap the metric computation to a single class.\n",
    "\n",
    "**YOUR TASK:** Finish the code of train.Metric class, and run the cell below.  **[3 mark(s)]**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from dataset import get_data_loader, move_data_to_device\n",
    "from hparams import Hparams\n",
    "from train import LossFunc, Metrics\n",
    "\n",
    "loss_func = LossFunc(device='cpu')\n",
    "metric = Metrics(loss_func=loss_func)\n",
    "\n",
    "# dummy output\n",
    "on_out = torch.rand(size=(8, 250))          # [B, T]\n",
    "off_out = torch.rand(size=(8, 250))\n",
    "octave_out = torch.rand(size=(8, 250, 5))   # [B, T, #Class]\n",
    "pitch_class_out = torch.rand(size=(8, 250, 13))\n",
    "out = (on_out, off_out, octave_out, pitch_class_out)\n",
    "\n",
    "train_loader = get_data_loader(split='train', args=Hparams.args)\n",
    "for i, data in enumerate(tqdm(train_loader)):\n",
    "    x, onset, offset, octave, pitch_class = move_data_to_device(data, 'cpu')\n",
    "    tgt = (onset, offset, octave, pitch_class)\n",
    "\n",
    "    metric.update(out, tgt)\n",
    "    if i == 4:\n",
    "        break\n",
    "train_metric = metric.get_value()\n",
    "print(train_metric, '\\n')\n",
    "assert len(train_metric) == 9\n",
    "for k in train_metric:\n",
    "    assert train_metric[k] > 0\n",
    "assert metric.buffer == {}\n",
    "print('Congrats!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 5: Model [5 marks]\n",
    "\n",
    "**YOUR TASK:**\n",
    "1. Implement the model in model.BaseCNN_mini, following the description below **[4 mark(s)]**\n",
    "2. Successfully run the cell below. **[1 mark(s)]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "**Model Description**\n",
    "1. This is a convolutional neural network that operates on spectrogram of a 5s-segment audio.\n",
    "2. Three 2-d convolutional layers at the beginning, each with **3x3 kernal size** and **1x1 padding**. **Output channel number: 16, 32, 64**.\n",
    "3. There is a batch normalization after each conv layer, then followed by ReLU as activation.\n",
    "4. Before the 2nd and 3rd conv layer, there are max pooling (**kernel size=(1,2)**) along the feature dimension. **NOTE:** do not shrink the time dimension, because we need to make prediction for each frame.\n",
    "5. After all convolution operation, permute the \"feature\" and \"channel\" dimensions so that they are adjacent, then merge the two dimensions to form a new feature dimension.\n",
    "6. There is a position-wise feed-forward layer with **256 dimention**, i.e., for all frames along the time axis, convert all features from each frame into a 256-d vector. There is a ReLU activation function afterwards.\n",
    "7. Prediction heads for onset, offset, octave, pitch class, each of them is a linear layer. They receive output from feed-forward layer, and produce the final output. **Note:** no activation function for these last layers, e.g., sigmoid/softmax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "from model import BaseCNN_mini\n",
    "\n",
    "model = BaseCNN_mini(feat_dim=256)\n",
    "dummy_input = torch.rand(size=(8, 250, 256))\n",
    "out = model(dummy_input)\n",
    "on, off, oct, pit = out\n",
    "\n",
    "assert list(on.shape) == [8, 250]\n",
    "assert list(off.shape) == [8, 250]\n",
    "assert list(oct.shape) == [8, 250, 5]\n",
    "assert list(pit.shape) == [8, 250, 13]\n",
    "\n",
    "print('Congrats!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Section 2 - Training and Evaluation [14 marks]\n",
    "\n",
    "### Task 6: Training [5 marks]\n",
    "Now we are ready for training! Both the model and data are not large, training can be performed in your laptop.\n",
    "Start training with the following command (estimated training time: 10 min):\n",
    "\n",
    "        python train.py\n",
    "You may need some time for debugging to successfully finish the training.\n",
    "\n",
    "### Task 7: Testing [3 marks]\n",
    "Then is testing. Test the model by\n",
    "\n",
    "        python test.py\n",
    "\n",
    "You may need to adjust the threshold values to make precision and recall to a similar value, to maximize the F1 score.\n",
    "\n",
    "The estimated performance: **48%, 38%, 17%, for COn, COnP, COnPOff**, respectively.\n",
    "- **COn** (*Correct Onset*): Percentage of notes with correctly predicted onset time within a tolerance window.  \n",
    "- **COnP** (*Correct Onset & Pitch*): Percentage of notes with both onset time and pitch correct.  \n",
    "- **COnPOff** (*Correct Onset, Pitch & Offset*): Percentage of notes with onset time, pitch, and note offset all correct.  \n",
    "\n",
    "\n",
    "After finishing training or testing, please attach a screenshot that indicate training/testing is finished. **Please include your command line prompt in the screenshot to show that is you**.\n",
    "\n",
    "### Task 8: Visualization (Case Study) [6 marks]\n",
    "Finally, to have an intuitive understanding of the model's performance, please visualize the output of your model and the ground truth (annotation), for one segment of audio. You may choose whichever way you would like. And after that, please also attach your visualization figure below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "[TODO: Please attach your screenshots in this block]\n",
    "\n",
    "**[Screenshot of finish of training]**\n",
    "\n",
    "**[Screenshot of finish of testing]**\n",
    "\n",
    "**[Visualization figure of output and ground truth]**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Section 3 - Questions [13 marks]\n",
    "**YOUR TASKS:** Please answer the questions below:\n",
    "\n",
    "-  Explain how the post-processing algorithm operates. Specifically, describe how it converts frame-level outputs into note-level outputs before the final evaluation. (You may need to locate and review the relevant code in the provided `.py` files before answering.) **[2 marks]**\n",
    "[Your answer]\n",
    "</br>\n",
    "\n",
    "- Describe how the final performance metric for note-level transcription is computed. Explain the evaluation process for a pair consisting of a note-level output and the corresponding annotation for one song. (Hint: review the relevant code, and consult the “Useful Resources” section at the beginning of this notebook.) **[2 marks]**\n",
    "[Your answer]\n",
    "</br>\n",
    "\n",
    "- Recall that the `Metrics` class uses the F1 score to evaluate onset/offset classification. Do you think the F1 score is appropriate in this case? What about using accuracy (correct predictions ÷ total frames) instead? If accuracy is not appropriate, explain why, and do the same for the F1 score. Suggest alternative metrics that may be more suitable. **[2 marks]**\n",
    "[Your answer]\n",
    "</br>\n",
    "\n",
    "- Summarize your system’s transcription performance (excluding time-efficiency) using objective metric scores and your visualization results. **[2 marks]** \n",
    "[Your answer]\n",
    "</br>\n",
    "\n",
    "- The current system may perform poorly on this dataset, or may achieve decent results but still have room for improvement. List three **pairs** of:  \n",
    "  1. A possible reason for suboptimal performance, and  \n",
    "  2. A corresponding direction for improvement. **[3 marks]**\n",
    "[Your answer] \n",
    "</br>\n",
    "\n",
    "- Which aspect of the assignment did you find most difficult? On which part did you spend the most time? **[1 mark]** \n",
    "[Your answer]\n",
    "</br>\n",
    "\n",
    "- How much time did you spend completing the assignment? If you did not record the exact duration, provide an estimate. **[1 mark(s)]**\n",
    "[Your answer]\n",
    "</br>\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
